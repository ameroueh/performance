{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we discuss various aspects of performance when programming in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiled vs interpreted language implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiled implementations of languages require the use of a *compiler*. This is a computer program which converts the human-readable *source file* written by a human into machine-readable code. The output is an *executable file*, which is a sequence of instructions which the central processing unit (CPU) of the computer is able to process and execute directly. Modern CPUs can execute billions of these elementary instructions every second. Because the instructions can directly be processed by the CPU, the executable is *architecture-dependent*. In other words, an executable compiled to run on your laptop will probably not run on your phone. The most common architectures today are x86_64 for computers and ARM for mobile and embedded devices.\n",
    "\n",
    "C++ is an ubiquitous compiled programming language; it was created to add object-oriented programming functionalities to C, and is designed for performance over all else. Let's look at a simple sample C++ file, and its translation into x86_64 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example0.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program defines three variables `a`, `b` and `c`, and stores the sum of `a` and `b` into `c`. It then displays the result. To compile it, we'll use `g++`, an open source C++ compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ example0.cpp -O0 -o example0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the compiled executable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./example0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *disassembler* decomposes a binary executable into individual instructions for humans to read (this may look like a simple task, but is not an exact science!). Let's disassemble the executable we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!objdump -d example0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot there, but we are only interested in the section starting with `<main>:` since it directly corresponds to the C++ code we wrote. Let's isolate that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!objdump -d example0 | sed -n '/<main>:/,/retq/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to recognise what we wrote in C++ after a while... for example, lines 908, 90f and 916 define the three variables, lines 91d and 920 push the contents of the variables to the CPU registers, and finally line 923 adds the value of the two registers! For more complicated programmes, this becomes tricky to follow, to say the least... but this is also the point: this code is only supposed to be read by a machine. However, we do see that the critical part of our programme was converted to a handful of CPU instructions, which is great for performance. If you'd like to get more hands-on with assembly code, here are three useful resources:\n",
    "* https://godbolt.org/: generates assembly code on the fly and displays it next to your C++ code. Very useful to learn assembly!\n",
    "* https://www.agner.org/optimize: contains, amongst other things, a guide on optimization for x86 processors (AMD, VIA and Intel). If you ever wondered what `mov`, `1ea` or `callq` do, what their throughput and latencies are on a given CPU, this is the place to go.\n",
    "* Intel publishes \"intrinsics\" i.e. C functions that map directly to individual CPU instructions. More here: https://software.intel.com/sites/landingpage/IntrinsicsGuide/#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: replace `-O0` by `-O3` in the cell `!g++ example0.cpp -O0 -o example0`, and re-run the cell. Reexamine the executable. What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreted language implementations function differently from compiled ones. Instead of being converted to CPU instructions directly, the source code is parsed then intermediate, machine-independent *opcode* (or *bytecode*) is generated. This intermediate code is then *interpreted* by an interpreter (for the Python language, the reference interpreter is CPython - so called because it is written in C - and is the program run when you call `python` on the command line). In a way, the interpreter is a \"machine within the machine\": it takes the place of the CPU (though of course it itself is being run by the CPU!). This explains some of the performance loss incurred when running Python code, since the presence of an interpreter induces overhead. We've written a Python program equivalent to the C++ one above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./example1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can explicitly ask the Python executable to generate the intermediate opcode. Ever wondered what the `.pyc` files present in `__pycache__` were?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m compileall example1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python view_bytecode.py __pycache__/example1.cpython-36.pyc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group of opcodes directly corresponds to each line in `example1.py` (and this is much more readable than assembly code...). To execute each opcode, many CPU instructions are required - but how many? To get a feel for this, let's inspect the source code of the CPython interpreter! In fact, let's just consider the crucial opcode, which is `BINARY_ADD` (adds two numbers together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line if you want to inspect more of the CPYthon source code than what is looked at in this notebook.\n",
    "# !git clone https://github.com/python/cpython.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to have a closer look at `ceval.c`. We'll only show the relevant parts, since it is a lot of C code to take in in one go. Effectively, you'll find a big `switch` statement, followed by a lot of `case` statements, each corresponding to one opcode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep 'case TARGET' cpython/Python/ceval.c | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is C code, it's not hard to see what this is doing: the programme is looping over each opcode, and based on the value of the opcode does someting different. So what happens when `BINARY_ADD` is encountered?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!sed -n '/BINARY_ADD/,/DISPATCH()/p' cpython/Python/ceval.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fairly complicated for a simple sum... but really all this does is call PyNumber_Add provided the arguments are not two Unicode strings (in which case they will be concatenated). So let's have a look at `PyNumber_Add`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '/PyNumber_Add/,/return result/p' cpython/Objects/abstract.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we call `binary_op1`. If the result is `Py_NotImplemented` we do... something, else we return the result. Fine, let's have a look at `binary_op1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n '/binary_op1/,/Py_RETURN_NOTIMPLEMENTED/p' cpython/Objects/abstract.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be doing some type checking. If the object in question a \"number type\" (if so define the `slotv` function...) and more checking, and finally if possible run `slotv`. And do make sure to check that the returned type is not `PyNotImplemented`, of course..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here is that Python is (very) dynamically typed: types are associated to *values* rather than *variables*. So whenever an operation is executed on an object, the interpreter has to check that the relevant object supports it. And take care of potential failures. Everything is an object in Python! What was one CPU instruction in our C++ example is quickly turning into many more when the Python interpreter is running the show. Because of the flexibility that Python offers (duck typing, etc), a lot of optimisations that are available to other interpreted languages are simply not available to it. That is the main reason why Python is so slow! It's common for a C++ implementation of a function to be 100x faster than a Python implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to explore the CPython source code in more details, why not use `gdb` (the GNU debugger) and step through the execution of the CPython interpreter? You can find a tutorial here: https://medium.com/@skabbass1/how-to-step-through-the-cpython-interpreter-2337da8a47ba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways in which you can improve the performance of your Python code. Normally, your programmes will spend most of their time in the same portion of the code - we call these *hotspots*. It is these hotspots that really need to be sped up, and not the rest of your code. For example, if your programme consists of reading a file from disk, parsing it into a custom data structure, and then running some bespoke ML algorithm on that structure, then probably only the latter step needs optimisation. We will now discuss three ways of speeding up your Python code: Cython, Numba and writing custom extensions. We won't discuss some others like using a different Python interpreter such as PyPy or Jython."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cython is an *optimising static compiler* for the Python language and the *extended Cython language, which is a superset of the Python programming language*. What does that mean? Cython compiles Python code, instead of interpreting it. Second, it defines a set of annotations that you may use on your Python code. These annotations are not part of the Python language. Rather, they help Cython understand the types of your variables and the signatures of your functions. Really, they let you write C code with Python syntax. The corollary here is that to use Cython well, you need to understand C. In particular, Cython is most succesful at optimising your code when you do not use Python's convenient features like dynamically typed variables, introspection, and so on. Let's give Cython a go. To compare its performance with pure Python, we'll test a function adding the first $10^8$ integers and returning the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat add_integers_python.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cython code should be written in files ending in `.pyx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat add_integers_cython.pyx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fairly similar to the pure Python function. The main difference is that we annoted the type of the variables using the `cdef` keyword, and also annotated the function arguments. Note that `uint64_t` stands for unsigned 64 bit integer. It can hold any non-negative integer between 0 and $2^{64} - 1$, inclusive. To compile `add_integers_cython.pyx` into a Python extension, we have written a custom `setup.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the extension as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the pure python implementation to the Cython one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timefunc(func, *args, **kwargs):\n",
    "    t0 = time.time()\n",
    "    func(*args, **kwargs)\n",
    "    t1 = time.time()\n",
    "    return t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_integers_python import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(f, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from add_integers_cython import f as f_cython\n",
    "from add_integers_cython import f_no_annotations as f_cython_no_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timefunc(f_cython_no_annotations, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timefunc(f_cython, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a roughly 200x improvement when using Cython annotations! When not annotating the code, Cython still provides a 2X performance improvement (recall that the function is statically compiled rather than interpreted). You may want to check out `add_integers_cython.c`. The `add_integers_cython.pyx` file was tranlated into this C source file, and the latter was then compiled using a C compiler. For more information on Cython, see https://cython.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba is easier to use than Cython. Numba's premise is that rather than having you specify types manually, it will try to infer them at runtime for you. As a consequence, most of the time all you have to do is to decorate your functions to tell numba to try to attempt and optimise them. No need for a different language here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_numba = jit(nopython=True)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_numba(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timefunc(f_numba, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_integers_numba import f_numba, f_numba_with_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(f_numba_with_signature, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_numba_with_signature(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That was fast. How is this possible? You may want to use the `inspect_asm()` or `inspect_llvm()` methods of `f_numba` to check your theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible, and sometimes necessary, to specify the signatures of your functions directly, rather than to let `numba` find out what they are on its own. That's how you do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import uint64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_numba_with_signature = jit(uint64(uint64), nopython=True)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_numba_with_signature(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba has (very basic) support for classes, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jitclass\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jitclass(OrderedDict([(\"x\", uint64), (\"y\", uint64)]))\n",
    "class Point2D:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some trickery is required if you want to specify the signature of functions using your jitclassed classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumbaPoint2DType = Point2D.class_type.instance_type\n",
    "\n",
    "@jit(NumbaPoint2DType(NumbaPoint2DType, NumbaPoint2DType), nopython=True)\n",
    "def add_points(a, b):\n",
    "    return Point2D(a.x + b.x, a.y + b.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = add_points(Point2D(1, 2), Point2D(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(result.x, result.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class support is limited, however. For example, it is not possible for a class to reference itself - something you would do using pointers in C++. So defining recursive structures is very difficult (I have seen one example online, and could not adapt it to my needs). Again, knowing some C is useful here to understand what can and cannot be done with `numba`. For more information, follow the documentation here: http://numba.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using C/C++ directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If C knowledge is effectively required to properly use Cython or Numba, might we not want to code our functions directly in C? Indeed, this is possible, and is probably preferable. One of the risks in using Cython or Numba is that they may not always suit your needs (see limitations above). You would not want to realise this in the middle of a project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The C Foreign Function Interface (`cffi`) package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cffi` package allows you to use compiled C libraries directly. Here is our favourite function coded up in C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat add_integers_c.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat add_integers_c.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compile it into a *shared library*. A shared library is a file containing machine code intended to be used by executable files. Shared libraries can be *statically linked*, meaning that their code is directly integrated into a single monolithic executable right after compilation, or can be *dynamically linked*, meaning that they are made available to executables at runtime. On Linux, static library names end in `.a` and dynamic libraries names end in `.so`. On Windows they end in `.lib` and `.dll`, respectively, and on OS X in `.a` and `.dylib`, respectively. It's often better to dynamically link your libraries. This makes it possible for only one instance of the shared code to be loaded into system memory, but most of all for the library code to be modified independently from the executable code (very useful when a bug is discovered!).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc -c -Wall -Werror -fPIC add_integers_c.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc -shared -Wl,-soname,libadd_integers_c.so -o libadd_integers_c.so add_integers_c.o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `cffi` to make functions from this shared library available from a Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "scrolled": false
    },
    "remote_metadata": {
     "scrolled": true
    }
   },
   "outputs": [],
   "source": [
    "from cffi import FFI\n",
    "\n",
    "ffibuilder = FFI()\n",
    "\n",
    "# cdef() expects a single string declaring the C types, functions and\n",
    "# globals needed to use the shared object. It must be in valid C syntax.\n",
    "ffibuilder.cdef(\n",
    "    \"\"\"\n",
    "    uint64_t f(uint64_t);\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# set_source() gives the name of the python extension module to\n",
    "# produce, and some C source code as a string.  This C code needs\n",
    "# to make the declarated functions, types and globals available,\n",
    "# so it is often just the \"#include\".\n",
    "ffibuilder.set_source(\n",
    "    \"_add_integers_cffi\",\n",
    "    \"\"\"\n",
    "     #include \"add_integers_c.h\"   // the C header of the library\n",
    "\"\"\",\n",
    "    libraries=[\"add_integers_c\"],\n",
    "    extra_link_args=[\n",
    "        \"-L/project/performance/examples\"\n",
    "    ],  # Must add this to tell the linker where to find our shared library\n",
    ")  # library name, for the linker\n",
    "\n",
    "ffibuilder.compile(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _add_integers_cffi import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.f(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a C++ extension with Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boost (https://www.boost.org) has a library which wraps all the boiler plate code required to write Python extensions manually (it will take care of reference counts for you, for example...). The result is that it is quite simple to expose your C++ classes as Python classes. Installation is a bit tricky, though. For reference, a list of instructions follows. However, we have included a script to automate this process. To run this script, close this notebook, open a new terminal and run\n",
    "```\n",
    "cd install-boost && ./install_boost.sh\n",
    "```\n",
    "This will restart Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual installation instructions:\n",
    "\n",
    "1. make sure that `numpy` is installed.  For example using `pip`: \n",
    "\n",
    "   ```\n",
    "   pip install numpy\n",
    "   ```\n",
    "\n",
    "2. Download the Boost C++ libraries version 1.69 available here:\n",
    "   https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.gz for\n",
    "   example by running\n",
    "    \n",
    "   ```\n",
    "   cd /tmp && wget https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.gz\n",
    "   ```\n",
    "\n",
    "3. Extract the Boost C++ libraries:\n",
    "\n",
    "   ```\n",
    "   cd /tmp && tar -xvf boost_1_69_0.tar.gz\n",
    "   ``` \n",
    "   \n",
    "   This creates a directory `boost_1_69_0`.\n",
    "   Define an environment variable named `BOOST_ROOT`\n",
    "   pointing to the root of your newly installed Boost distribution:\n",
    "   \n",
    "   ```\n",
    "   export BOOST_ROOT=/tmp/boost_1_69_0\n",
    "   ```\n",
    "\n",
    "4. Compile the Boost Python and Numpy shared libraries. Run \n",
    "\n",
    "\t```\n",
    "\tcd $BOOST_ROOT && ./bootstrap.sh\n",
    "\t```\n",
    "\t\n",
    "   This will create a `bjam` configuration file named `project-config.jam`.\n",
    "   Bjam (the Boost build tool) is not always capable of detecting the correct\n",
    "   Python include paths, so we'll need to fix `project-config.jam` manually.\n",
    "   Look for a line resembling \n",
    "   ```\n",
    "   using python : 3.6 : /opt/anaconda/envs/Python3 ;\n",
    "   ```\n",
    "   in this file, and replace it with\n",
    "    \n",
    "   ```\n",
    "   using python : 3.6 : /opt/anaconda/envs/Python3 : /opt/anaconda/envs/Python3/include/python3.6m : /opt/anaconda/envs/Python3/lib ;\n",
    "   ``` \n",
    "   \n",
    "   If you are not using Faculty, if you are\n",
    "   targetting another version of Python, or if your Python installation is\n",
    "   located elsewhere, you will need to modify this step accordingly.  The\n",
    "   essential point is to make sure that the fourth field points to the directory\n",
    "   containing the Python C header files (in particular, it should contain the\n",
    "   file `Python.h`).  Now compile the Boost Python library with\n",
    "\n",
    "   ```\n",
    "   ./b2 install --with-python stage\n",
    "   ```\n",
    "   \n",
    "   The Boost Python and Numpy shared libraries will\n",
    "   be installed in `$BOOST_ROOT/stage/lib`.\n",
    "\n",
    "Let's also set a number of environment variables to make things tidier.\n",
    "\n",
    "* `BOOST_ROOT`: we have already defined this variable.\n",
    "  It should point to the directory containing the Boost C++ headers.\n",
    "\n",
    "* `BOOST_LIB`: this variable should point to the directory containing the shared\n",
    "  Boost Python and Boost Numpy libraries. To define it, run \n",
    "  \n",
    "  ```\n",
    "  export BOOST_LIB=$BOOST_ROOT/stage/lib\n",
    "  ```\n",
    "\n",
    "* `PYTHON_INCLUDE`: this variable should point to the directory containing the\n",
    "  Python header files. On Faculty, this would currently be set as follows:\n",
    "  \n",
    "  ```\n",
    "  export PYTHON_INCLUDE=/opt/anaconda/envs/Python3/include/python3.6m\n",
    "  ```\n",
    "\n",
    "* `NUMPY_INCLUDE`: this variable should point to the directory containing the\n",
    "  numpy header files.  On Faculty, this would currently be set as follows:\n",
    "\n",
    "  ```\n",
    "  export NUMPY_INCLUDE=/opt/anaconda/envs/Python3/lib/python3.6/site-packages/numpy/core/include\n",
    "  ```\n",
    "\n",
    "Additionally, `$BOOST_LIB` should be part of your `$LD_LIBRARY_PATH` environment\n",
    "variable. This is to make sure that the Python interpreter is able to find the\n",
    "shared Boost Python library we just compiled. Run the following\n",
    "command to make sure that `LD_LIBRARY_PATH` is correctly set:\n",
    "\n",
    "``` \n",
    "export LD_LIBRARY_PATH=$BOOST_LIB:$LD_LIBRARY_PATH\n",
    "```\n",
    "\n",
    "Note that this is the only environment variable that is required to be set\n",
    "after installation. On Faculty, you may want to create a file `envs.sh` in \n",
    "`/etc/faculty_environment.d` defining the variables above. Then restart Jupyter with \n",
    "```\n",
    "sudo sv stop jupyter && sudo sv start jupyter\n",
    "```\n",
    "The IPython kernels run by your Jupyter server will now have the variables correctly set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give Boost Python a go! Here is a simple example defining our function `f` as part of a module `add_integers_boost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat add_integers_boost.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ add_integers_boost.cpp \\\n",
    "     -fPIC \\\n",
    "     -shared \\\n",
    "     -I $BOOST_ROOT \\\n",
    "     -I $PYTHON_INCLUDE \\\n",
    "     -L $BOOST_LIB \\\n",
    "     -lboost_python36 \\\n",
    "     -Wno-deprecated-declarations \\\n",
    "     -o add_integers_boost.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word on the options:\n",
    "* `-fPIC` tells the compiler to generate position independent code, meaning that the code does not rely on where it is located in memory to be run. For example, jumps will be relative rather than absolute. This is required for shared libraries.\n",
    "* `-shared` tells the compiler to create a shared library.\n",
    "* `-I $BOOST_ROOT`, `-I $PYTHON_INCLUDE` and `-I $BOOST_LIB` give the compiler additional directories to look for header files\n",
    "* `-lboost_python36` tells the compiler to dynamically link the compiled library against the Boost Python library (for version 3.6)\n",
    "* `-Wno-deprecated-declarations` removes some warnings coming from the Boost headers (this can be omitted)\n",
    "* `-o add_integers_boost.so` specifies the output file name of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_integers_boost import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a C++ class exposed as a Python class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat accumulator_boost.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ accumulator_boost.cpp \\\n",
    "     -fPIC \\\n",
    "     -shared \\\n",
    "     -I $BOOST_ROOT \\\n",
    "     -I $PYTHON_INCLUDE \\\n",
    "     -L $BOOST_LIB \\\n",
    "     -lboost_python36 \\\n",
    "     -Wno-deprecated-declarations \\\n",
    "     -o accumulator_boost.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accumulator_boost import Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accumulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.add(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You typically will only want to implement the critical code paths in C++ - and hence only part of your class in C++. Therefore you might want to wrap the Boost-generated class in another class. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accumulator_boost import Accumulator as CoreAccumulator\n",
    "\n",
    "\n",
    "class Accumulator(CoreAccumulator):\n",
    "    def __init__(self):\n",
    "        self._core = CoreAccumulator()\n",
    "\n",
    "    def add(self, increment):\n",
    "        self._core.add(increment)\n",
    "\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self._core.total\n",
    "\n",
    "    def help(self):\n",
    "        print(\"Use this class to accumulate numbers.\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Accumulator: total = {}\".format(self.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accumulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.add(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although quite bare, the Boost Python documentation does have a helpful tutorial which you might want to check out - see the Boost documentation. In particular, it will show you how to interact with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can I code a C extension from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. Good luck :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about parallelisation... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System processes and threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing a *process* is an instance of program being executed. At any one time, your computer is running tens of processes, if not hundreds. Here's how to get the number of running user processes on Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ps -A --no-headers | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you are very likely to be running (many) more processes than you have CPU cores! How is this possible? In effect, only one process is ever being run at a time on a machine with a single core. Processes *share CPU time*: the CPU constantly switches between them, and this gives the illusion that processes are being run concurrently. The part of your operating system responsible for orchestrating this is called the *scheduler*. It divides time into slices, and decides which process should be run during each slice. Of course, when you have more than one core, the scheduler will use them all: processes will be scheduled to run on different cores.\n",
    "\n",
    " A process can run one or more *threads*: these are sequences of execution sharing the same address space (so that communication between threads is much faster than communication between processes). So your system runs many processes, and each process can run many threads. Again, there is no relationship between number of cores and number of threads, but having more cores means that threads can run concurrently. To speed up the performance of you programmes, you might therefore want to make them *multithreaded*. This typically will make your programmes run faster on multi-core machines, but not always! It all depends on whether the bottleneck in the execution of your programme is the CPU, or something else. To give an example, if your programme is transcoding a video, then the bottleneck is the CPU - we say that it is *CPU-bound*. Video transcoders are typically multithreaded, and this significantly improves performance on multi-core machines. If your programme is downloading data from the web, then its performance is probably limited by the network bandwidth available to you - it is *I/O-bound*. In this context, the CPU is probably spending most of its time idling and waiting for packets to come from the source. Dividing the downloading task into two threads would probably not speed things up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `threading` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create threads in Python with the `threading` package. Run the following cells on a machine with at least two cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_integers_python import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_threaded():\n",
    "    return f(100_000_000) + f(100_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(single_threaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_adapted(index, results, *args, **kwargs):\n",
    "    results[index] = f(*args, **kwargs)\n",
    "\n",
    "def multi_threaded():\n",
    "    results = {}\n",
    "    t0 = threading.Thread(target=f_adapted, args=(0, results, 100_000_000))\n",
    "    t1 = threading.Thread(target=f_adapted, args=(1, results, 100_000_000))\n",
    "    t0.start()\n",
    "    t1.start()\n",
    "    t0.join()\n",
    "    t1.join()\n",
    "    return results[0] + results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(multi_threaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was not much faster than the single-threaded version... in fact, it may even be a bit slower! Here is the problem:\n",
    "Python threads *cannot be run concurrently*. This is a feature of the Python interpreter - the same programme written in C\n",
    "would indeed be twice as fast as the single-threaded version. The Python interpreter has a so-called *Global Intepreter Lock (GIL)* - when a thread is being run by the interpreter, the GIL is set, which prevents other threads from running. In effect, the Python interpreter works like a single-core virtual machine. More on this here: https://www.dabeaz.com/python/UnderstandingGIL.pdf\n",
    "\n",
    "This does not mean that using threads in Python is not a good idea: they can be very useful if your programme has to run independent streams of code concurrently: for example to handle multiple connections to a server at the same time. But you shouldn't expect performance improvements on CPU-bound tasks from using Python threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `multiprocessing` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python does have a way of making use of multicore machines: the `multiprocessing` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_processed():\n",
    "    manager = multiprocessing.Manager()\n",
    "    results = manager.dict()\n",
    "    t0 = multiprocessing.Process(\n",
    "        target=f_adapted, args=(0, results, 100_000_000)\n",
    "    )\n",
    "    t1 = multiprocessing.Process(\n",
    "        target=f_adapted, args=(1, results, 100_000_000)\n",
    "    )\n",
    "    t0.start()\n",
    "    t1.start()\n",
    "    t0.join()\n",
    "    t1.join()\n",
    "    return results[0] + results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(multi_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *does* improve performance on multicore machines. As the name indicates, the way that this module works is by creating independent *processes* rather than *threads*: a distinct Python intepreter (and hence a distinct system process) will be run for each \"process\" that you create in this way. This means, in particular,  that these processes do not share the same address space. This works well only when a small amount of inter-process communication is required. Besides, running a new Python interpreter for each process has its own inconveniences and overheads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, if you choose to write a C/C++ extension (see above) then you can easily bypass the GIL mechanism, and you won't be limited by the `threading` or `multiprocessing` modules. And how does the (in?)famous `joblib` module come into the picture? Joblib will either use `threading` or `multiprocessing` depending on which backend you specify - and hence suffers from the same limitations as these modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many thanks to Scott Stevenson, Ben Green and Markus Kunesch for reviewing an earlier version of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "7"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "4"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
