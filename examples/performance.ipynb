{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we discuss various aspects of performance when coding in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiled vs interpreted languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiled languages require the use of a *compiler*. This is a computer program which converts the human-readable *source file* written by a human into machine-readable code. The output is an *executable file*, wwhich is a sequence of instruction which the central processing unit (CPU) of the computer is able to process and execute directly. Modern CPUs can execute billions of these elementary instructions every second. Because the instructions can directly be processed by the CPU, the executable is *architecture-dependent*. In other words, an executable compiled to run on your laptop will probably not run on your phone. The most common architectures today are x86-64 for computers and ARM for mobile and embedded devices. \n",
    "\n",
    "C++ is an ubiquitous compiled programming language; it was created to add object-oriented programming functionalities to C, which is even more widely used. Let's look at a simple sample C++ file, and its translation into x86-64 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <iostream>\r\n",
      "#include <string>\r\n",
      "\r\n",
      "int main() {\r\n",
      "\tint a = 5;\r\n",
      "\tint b = 8;\r\n",
      "\tint c = 0;\r\n",
      "\tc = a + b;\r\n",
      "\tstd::cout\r\n",
      "\t  << c << std::endl;\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat example0.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program defines three variables `a`, `b` and `c`, and stores the sum of `a` and `b` into `c`. It then displays the result. To compile it, we'll use `gcc`, the GNU Compiler Collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ example0.cpp -O0 -o example0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the compiled executable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\r\n"
     ]
    }
   ],
   "source": [
    "!./example0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *disassembler* decomposes a binary executable into individual instructions for humans to read (this may look like a simple task, but is not an exact science!). Let's disassemble the executable we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "example0:     file format elf64-x86-64\r\n",
      "\r\n",
      "\r\n",
      "Disassembly of section .init:\r\n",
      "\r\n",
      "0000000000000708 <_init>:\r\n",
      " 708:\t48 83 ec 08          \tsub    $0x8,%rsp\r\n",
      " 70c:\t48 8b 05 d5 08 20 00 \tmov    0x2008d5(%rip),%rax        # 200fe8 <__gmon_start__>\r\n",
      " 713:\t48 85 c0             \ttest   %rax,%rax\r\n",
      " 716:\t74 02                \tje     71a <_init+0x12>\r\n",
      " 718:\tff d0                \tcallq  *%rax\r\n",
      " 71a:\t48 83 c4 08          \tadd    $0x8,%rsp\r\n",
      " 71e:\tc3                   \tretq   \r\n",
      "\r\n",
      "Disassembly of section .plt:\r\n",
      "\r\n",
      "0000000000000720 <.plt>:\r\n",
      " 720:\tff 35 72 08 20 00    \tpushq  0x200872(%rip)        # 200f98 <_GLOBAL_OFFSET_TABLE_+0x8>\r\n",
      " 726:\tff 25 74 08 20 00    \tjmpq   *0x200874(%rip)        # 200fa0 <_GLOBAL_OFFSET_TABLE_+0x10>\r\n",
      " 72c:\t0f 1f 40 00          \tnopl   0x0(%rax)\r\n",
      "\r\n",
      "0000000000000730 <__cxa_atexit@plt>:\r\n",
      " 730:\tff 25 72 08 20 00    \tjmpq   *0x200872(%rip)        # 200fa8 <__cxa_atexit@GLIBC_2.2.5>\r\n",
      " 736:\t68 00 00 00 00       \tpushq  $0x0\r\n",
      " 73b:\te9 e0 ff ff ff       \tjmpq   720 <.plt>\r\n",
      "\r\n",
      "0000000000000740 <_ZNSolsEPFRSoS_E@plt>:\r\n",
      " 740:\tff 25 6a 08 20 00    \tjmpq   *0x20086a(%rip)        # 200fb0 <_ZNSolsEPFRSoS_E@GLIBCXX_3.4>\r\n",
      " 746:\t68 01 00 00 00       \tpushq  $0x1\r\n",
      " 74b:\te9 d0 ff ff ff       \tjmpq   720 <.plt>\r\n",
      "\r\n",
      "0000000000000750 <_ZNSt8ios_base4InitC1Ev@plt>:\r\n",
      " 750:\tff 25 62 08 20 00    \tjmpq   *0x200862(%rip)        # 200fb8 <_ZNSt8ios_base4InitC1Ev@GLIBCXX_3.4>\r\n",
      " 756:\t68 02 00 00 00       \tpushq  $0x2\r\n",
      " 75b:\te9 c0 ff ff ff       \tjmpq   720 <.plt>\r\n",
      "\r\n",
      "0000000000000760 <_ZNSolsEi@plt>:\r\n",
      " 760:\tff 25 5a 08 20 00    \tjmpq   *0x20085a(%rip)        # 200fc0 <_ZNSolsEi@GLIBCXX_3.4>\r\n",
      " 766:\t68 03 00 00 00       \tpushq  $0x3\r\n",
      " 76b:\te9 b0 ff ff ff       \tjmpq   720 <.plt>\r\n",
      "\r\n",
      "Disassembly of section .plt.got:\r\n",
      "\r\n",
      "0000000000000770 <__cxa_finalize@plt>:\r\n",
      " 770:\tff 25 52 08 20 00    \tjmpq   *0x200852(%rip)        # 200fc8 <__cxa_finalize@GLIBC_2.2.5>\r\n",
      " 776:\t66 90                \txchg   %ax,%ax\r\n",
      "\r\n",
      "Disassembly of section .text:\r\n",
      "\r\n",
      "0000000000000780 <_start>:\r\n",
      " 780:\t31 ed                \txor    %ebp,%ebp\r\n",
      " 782:\t49 89 d1             \tmov    %rdx,%r9\r\n",
      " 785:\t5e                   \tpop    %rsi\r\n",
      " 786:\t48 89 e2             \tmov    %rsp,%rdx\r\n",
      " 789:\t48 83 e4 f0          \tand    $0xfffffffffffffff0,%rsp\r\n",
      " 78d:\t50                   \tpush   %rax\r\n",
      " 78e:\t54                   \tpush   %rsp\r\n",
      " 78f:\t4c 8d 05 1a 02 00 00 \tlea    0x21a(%rip),%r8        # 9b0 <__libc_csu_fini>\r\n",
      " 796:\t48 8d 0d a3 01 00 00 \tlea    0x1a3(%rip),%rcx        # 940 <__libc_csu_init>\r\n",
      " 79d:\t48 8d 3d e6 00 00 00 \tlea    0xe6(%rip),%rdi        # 88a <main>\r\n",
      " 7a4:\tff 15 36 08 20 00    \tcallq  *0x200836(%rip)        # 200fe0 <__libc_start_main@GLIBC_2.2.5>\r\n",
      " 7aa:\tf4                   \thlt    \r\n",
      " 7ab:\t0f 1f 44 00 00       \tnopl   0x0(%rax,%rax,1)\r\n",
      "\r\n",
      "00000000000007b0 <deregister_tm_clones>:\r\n",
      " 7b0:\t48 8d 3d 59 08 20 00 \tlea    0x200859(%rip),%rdi        # 201010 <__TMC_END__>\r\n",
      " 7b7:\t55                   \tpush   %rbp\r\n",
      " 7b8:\t48 8d 05 51 08 20 00 \tlea    0x200851(%rip),%rax        # 201010 <__TMC_END__>\r\n",
      " 7bf:\t48 39 f8             \tcmp    %rdi,%rax\r\n",
      " 7c2:\t48 89 e5             \tmov    %rsp,%rbp\r\n",
      " 7c5:\t74 19                \tje     7e0 <deregister_tm_clones+0x30>\r\n",
      " 7c7:\t48 8b 05 0a 08 20 00 \tmov    0x20080a(%rip),%rax        # 200fd8 <_ITM_deregisterTMCloneTable>\r\n",
      " 7ce:\t48 85 c0             \ttest   %rax,%rax\r\n",
      " 7d1:\t74 0d                \tje     7e0 <deregister_tm_clones+0x30>\r\n",
      " 7d3:\t5d                   \tpop    %rbp\r\n",
      " 7d4:\tff e0                \tjmpq   *%rax\r\n",
      " 7d6:\t66 2e 0f 1f 84 00 00 \tnopw   %cs:0x0(%rax,%rax,1)\r\n",
      " 7dd:\t00 00 00 \r\n",
      " 7e0:\t5d                   \tpop    %rbp\r\n",
      " 7e1:\tc3                   \tretq   \r\n",
      " 7e2:\t0f 1f 40 00          \tnopl   0x0(%rax)\r\n",
      " 7e6:\t66 2e 0f 1f 84 00 00 \tnopw   %cs:0x0(%rax,%rax,1)\r\n",
      " 7ed:\t00 00 00 \r\n",
      "\r\n",
      "00000000000007f0 <register_tm_clones>:\r\n",
      " 7f0:\t48 8d 3d 19 08 20 00 \tlea    0x200819(%rip),%rdi        # 201010 <__TMC_END__>\r\n",
      " 7f7:\t48 8d 35 12 08 20 00 \tlea    0x200812(%rip),%rsi        # 201010 <__TMC_END__>\r\n",
      " 7fe:\t55                   \tpush   %rbp\r\n",
      " 7ff:\t48 29 fe             \tsub    %rdi,%rsi\r\n",
      " 802:\t48 89 e5             \tmov    %rsp,%rbp\r\n",
      " 805:\t48 c1 fe 03          \tsar    $0x3,%rsi\r\n",
      " 809:\t48 89 f0             \tmov    %rsi,%rax\r\n",
      " 80c:\t48 c1 e8 3f          \tshr    $0x3f,%rax\r\n",
      " 810:\t48 01 c6             \tadd    %rax,%rsi\r\n",
      " 813:\t48 d1 fe             \tsar    %rsi\r\n",
      " 816:\t74 18                \tje     830 <register_tm_clones+0x40>\r\n",
      " 818:\t48 8b 05 d1 07 20 00 \tmov    0x2007d1(%rip),%rax        # 200ff0 <_ITM_registerTMCloneTable>\r\n",
      " 81f:\t48 85 c0             \ttest   %rax,%rax\r\n",
      " 822:\t74 0c                \tje     830 <register_tm_clones+0x40>\r\n",
      " 824:\t5d                   \tpop    %rbp\r\n",
      " 825:\tff e0                \tjmpq   *%rax\r\n",
      " 827:\t66 0f 1f 84 00 00 00 \tnopw   0x0(%rax,%rax,1)\r\n",
      " 82e:\t00 00 \r\n",
      " 830:\t5d                   \tpop    %rbp\r\n",
      " 831:\tc3                   \tretq   \r\n",
      " 832:\t0f 1f 40 00          \tnopl   0x0(%rax)\r\n",
      " 836:\t66 2e 0f 1f 84 00 00 \tnopw   %cs:0x0(%rax,%rax,1)\r\n",
      " 83d:\t00 00 00 \r\n",
      "\r\n",
      "0000000000000840 <__do_global_dtors_aux>:\r\n",
      " 840:\t80 3d e9 08 20 00 00 \tcmpb   $0x0,0x2008e9(%rip)        # 201130 <completed.7696>\r\n",
      " 847:\t75 2f                \tjne    878 <__do_global_dtors_aux+0x38>\r\n",
      " 849:\t48 83 3d 77 07 20 00 \tcmpq   $0x0,0x200777(%rip)        # 200fc8 <__cxa_finalize@GLIBC_2.2.5>\r\n",
      " 850:\t00 \r\n",
      " 851:\t55                   \tpush   %rbp\r\n",
      " 852:\t48 89 e5             \tmov    %rsp,%rbp\r\n",
      " 855:\t74 0c                \tje     863 <__do_global_dtors_aux+0x23>\r\n",
      " 857:\t48 8b 3d aa 07 20 00 \tmov    0x2007aa(%rip),%rdi        # 201008 <__dso_handle>\r\n",
      " 85e:\te8 0d ff ff ff       \tcallq  770 <__cxa_finalize@plt>\r\n",
      " 863:\te8 48 ff ff ff       \tcallq  7b0 <deregister_tm_clones>\r\n",
      " 868:\tc6 05 c1 08 20 00 01 \tmovb   $0x1,0x2008c1(%rip)        # 201130 <completed.7696>\r\n",
      " 86f:\t5d                   \tpop    %rbp\r\n",
      " 870:\tc3                   \tretq   \r\n",
      " 871:\t0f 1f 80 00 00 00 00 \tnopl   0x0(%rax)\r\n",
      " 878:\tf3 c3                \trepz retq \r\n",
      " 87a:\t66 0f 1f 44 00 00    \tnopw   0x0(%rax,%rax,1)\r\n",
      "\r\n",
      "0000000000000880 <frame_dummy>:\r\n",
      " 880:\t55                   \tpush   %rbp\r\n",
      " 881:\t48 89 e5             \tmov    %rsp,%rbp\r\n",
      " 884:\t5d                   \tpop    %rbp\r\n",
      " 885:\te9 66 ff ff ff       \tjmpq   7f0 <register_tm_clones>\r\n",
      "\r\n",
      "000000000000088a <main>:\r\n",
      " 88a:\t55                   \tpush   %rbp\r\n",
      " 88b:\t48 89 e5             \tmov    %rsp,%rbp\r\n",
      " 88e:\t48 83 ec 10          \tsub    $0x10,%rsp\r\n",
      " 892:\tc7 45 f4 05 00 00 00 \tmovl   $0x5,-0xc(%rbp)\r\n",
      " 899:\tc7 45 f8 08 00 00 00 \tmovl   $0x8,-0x8(%rbp)\r\n",
      " 8a0:\tc7 45 fc 00 00 00 00 \tmovl   $0x0,-0x4(%rbp)\r\n",
      " 8a7:\t8b 55 f4             \tmov    -0xc(%rbp),%edx\r\n",
      " 8aa:\t8b 45 f8             \tmov    -0x8(%rbp),%eax\r\n",
      " 8ad:\t01 d0                \tadd    %edx,%eax\r\n",
      " 8af:\t89 45 fc             \tmov    %eax,-0x4(%rbp)\r\n",
      " 8b2:\t8b 45 fc             \tmov    -0x4(%rbp),%eax\r\n",
      " 8b5:\t89 c6                \tmov    %eax,%esi\r\n",
      " 8b7:\t48 8d 3d 62 07 20 00 \tlea    0x200762(%rip),%rdi        # 201020 <_ZSt4cout@@GLIBCXX_3.4>\r\n",
      " 8be:\te8 9d fe ff ff       \tcallq  760 <_ZNSolsEi@plt>\r\n",
      " 8c3:\t48 89 c2             \tmov    %rax,%rdx\r\n",
      " 8c6:\t48 8b 05 03 07 20 00 \tmov    0x200703(%rip),%rax        # 200fd0 <_ZSt4endlIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_@GLIBCXX_3.4>\r\n",
      " 8cd:\t48 89 c6             \tmov    %rax,%rsi\r\n",
      " 8d0:\t48 89 d7             \tmov    %rdx,%rdi\r\n",
      " 8d3:\te8 68 fe ff ff       \tcallq  740 <_ZNSolsEPFRSoS_E@plt>\r\n",
      " 8d8:\tb8 00 00 00 00       \tmov    $0x0,%eax\r\n",
      " 8dd:\tc9                   \tleaveq \r\n",
      " 8de:\tc3                   \tretq   \r\n",
      "\r\n",
      "00000000000008df <_Z41__static_initialization_and_destruction_0ii>:\r\n",
      " 8df:\t55                   \tpush   %rbp\r\n",
      " 8e0:\t48 89 e5             \tmov    %rsp,%rbp\r\n",
      " 8e3:\t48 83 ec 10          \tsub    $0x10,%rsp\r\n",
      " 8e7:\t89 7d fc             \tmov    %edi,-0x4(%rbp)\r\n",
      " 8ea:\t89 75 f8             \tmov    %esi,-0x8(%rbp)\r\n",
      " 8ed:\t83 7d fc 01          \tcmpl   $0x1,-0x4(%rbp)\r\n",
      " 8f1:\t75 32                \tjne    925 <_Z41__static_initialization_and_destruction_0ii+0x46>\r\n",
      " 8f3:\t81 7d f8 ff ff 00 00 \tcmpl   $0xffff,-0x8(%rbp)\r\n",
      " 8fa:\t75 29                \tjne    925 <_Z41__static_initialization_and_destruction_0ii+0x46>\r\n",
      " 8fc:\t48 8d 3d 2e 08 20 00 \tlea    0x20082e(%rip),%rdi        # 201131 <_ZStL8__ioinit>\r\n",
      " 903:\te8 48 fe ff ff       \tcallq  750 <_ZNSt8ios_base4InitC1Ev@plt>\r\n",
      " 908:\t48 8d 15 f9 06 20 00 \tlea    0x2006f9(%rip),%rdx        # 201008 <__dso_handle>\r\n",
      " 90f:\t48 8d 35 1b 08 20 00 \tlea    0x20081b(%rip),%rsi        # 201131 <_ZStL8__ioinit>\r\n",
      " 916:\t48 8b 05 db 06 20 00 \tmov    0x2006db(%rip),%rax        # 200ff8 <_ZNSt8ios_base4InitD1Ev@GLIBCXX_3.4>\r\n",
      " 91d:\t48 89 c7             \tmov    %rax,%rdi\r\n",
      " 920:\te8 0b fe ff ff       \tcallq  730 <__cxa_atexit@plt>\r\n",
      " 925:\t90                   \tnop\r\n",
      " 926:\tc9                   \tleaveq \r\n",
      " 927:\tc3                   \tretq   \r\n",
      "\r\n",
      "0000000000000928 <_GLOBAL__sub_I_main>:\r\n",
      " 928:\t55                   \tpush   %rbp\r\n",
      " 929:\t48 89 e5             \tmov    %rsp,%rbp\r\n",
      " 92c:\tbe ff ff 00 00       \tmov    $0xffff,%esi\r\n",
      " 931:\tbf 01 00 00 00       \tmov    $0x1,%edi\r\n",
      " 936:\te8 a4 ff ff ff       \tcallq  8df <_Z41__static_initialization_and_destruction_0ii>\r\n",
      " 93b:\t5d                   \tpop    %rbp\r\n",
      " 93c:\tc3                   \tretq   \r\n",
      " 93d:\t0f 1f 00             \tnopl   (%rax)\r\n",
      "\r\n",
      "0000000000000940 <__libc_csu_init>:\r\n",
      " 940:\t41 57                \tpush   %r15\r\n",
      " 942:\t41 56                \tpush   %r14\r\n",
      " 944:\t49 89 d7             \tmov    %rdx,%r15\r\n",
      " 947:\t41 55                \tpush   %r13\r\n",
      " 949:\t41 54                \tpush   %r12\r\n",
      " 94b:\t4c 8d 25 26 04 20 00 \tlea    0x200426(%rip),%r12        # 200d78 <__frame_dummy_init_array_entry>\r\n",
      " 952:\t55                   \tpush   %rbp\r\n",
      " 953:\t48 8d 2d 2e 04 20 00 \tlea    0x20042e(%rip),%rbp        # 200d88 <__init_array_end>\r\n",
      " 95a:\t53                   \tpush   %rbx\r\n",
      " 95b:\t41 89 fd             \tmov    %edi,%r13d\r\n",
      " 95e:\t49 89 f6             \tmov    %rsi,%r14\r\n",
      " 961:\t4c 29 e5             \tsub    %r12,%rbp\r\n",
      " 964:\t48 83 ec 08          \tsub    $0x8,%rsp\r\n",
      " 968:\t48 c1 fd 03          \tsar    $0x3,%rbp\r\n",
      " 96c:\te8 97 fd ff ff       \tcallq  708 <_init>\r\n",
      " 971:\t48 85 ed             \ttest   %rbp,%rbp\r\n",
      " 974:\t74 20                \tje     996 <__libc_csu_init+0x56>\r\n",
      " 976:\t31 db                \txor    %ebx,%ebx\r\n",
      " 978:\t0f 1f 84 00 00 00 00 \tnopl   0x0(%rax,%rax,1)\r\n",
      " 97f:\t00 \r\n",
      " 980:\t4c 89 fa             \tmov    %r15,%rdx\r\n",
      " 983:\t4c 89 f6             \tmov    %r14,%rsi\r\n",
      " 986:\t44 89 ef             \tmov    %r13d,%edi\r\n",
      " 989:\t41 ff 14 dc          \tcallq  *(%r12,%rbx,8)\r\n",
      " 98d:\t48 83 c3 01          \tadd    $0x1,%rbx\r\n",
      " 991:\t48 39 dd             \tcmp    %rbx,%rbp\r\n",
      " 994:\t75 ea                \tjne    980 <__libc_csu_init+0x40>\r\n",
      " 996:\t48 83 c4 08          \tadd    $0x8,%rsp\r\n",
      " 99a:\t5b                   \tpop    %rbx\r\n",
      " 99b:\t5d                   \tpop    %rbp\r\n",
      " 99c:\t41 5c                \tpop    %r12\r\n",
      " 99e:\t41 5d                \tpop    %r13\r\n",
      " 9a0:\t41 5e                \tpop    %r14\r\n",
      " 9a2:\t41 5f                \tpop    %r15\r\n",
      " 9a4:\tc3                   \tretq   \r\n",
      " 9a5:\t90                   \tnop\r\n",
      " 9a6:\t66 2e 0f 1f 84 00 00 \tnopw   %cs:0x0(%rax,%rax,1)\r\n",
      " 9ad:\t00 00 00 \r\n",
      "\r\n",
      "00000000000009b0 <__libc_csu_fini>:\r\n",
      " 9b0:\tf3 c3                \trepz retq \r\n",
      "\r\n",
      "Disassembly of section .fini:\r\n",
      "\r\n",
      "00000000000009b4 <_fini>:\r\n",
      " 9b4:\t48 83 ec 08          \tsub    $0x8,%rsp\r\n",
      " 9b8:\t48 83 c4 08          \tadd    $0x8,%rsp\r\n",
      " 9bc:\tc3                   \tretq   \r\n"
     ]
    }
   ],
   "source": [
    "!objdump -d example0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot there, but we are only interested in the section starting with `<main>:` since it directly corresponds to the C++ code we wrote. Let's isolate that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000088a <main>:\r\n",
      " 88a:\t55                   \tpush   %rbp\r\n",
      " 88b:\t48 89 e5             \tmov    %rsp,%rbp\r\n",
      " 88e:\t48 83 ec 10          \tsub    $0x10,%rsp\r\n",
      " 892:\tc7 45 f4 05 00 00 00 \tmovl   $0x5,-0xc(%rbp)\r\n",
      " 899:\tc7 45 f8 08 00 00 00 \tmovl   $0x8,-0x8(%rbp)\r\n",
      " 8a0:\tc7 45 fc 00 00 00 00 \tmovl   $0x0,-0x4(%rbp)\r\n",
      " 8a7:\t8b 55 f4             \tmov    -0xc(%rbp),%edx\r\n",
      " 8aa:\t8b 45 f8             \tmov    -0x8(%rbp),%eax\r\n",
      " 8ad:\t01 d0                \tadd    %edx,%eax\r\n",
      " 8af:\t89 45 fc             \tmov    %eax,-0x4(%rbp)\r\n",
      " 8b2:\t8b 45 fc             \tmov    -0x4(%rbp),%eax\r\n",
      " 8b5:\t89 c6                \tmov    %eax,%esi\r\n",
      " 8b7:\t48 8d 3d 62 07 20 00 \tlea    0x200762(%rip),%rdi        # 201020 <_ZSt4cout@@GLIBCXX_3.4>\r\n",
      " 8be:\te8 9d fe ff ff       \tcallq  760 <_ZNSolsEi@plt>\r\n",
      " 8c3:\t48 89 c2             \tmov    %rax,%rdx\r\n",
      " 8c6:\t48 8b 05 03 07 20 00 \tmov    0x200703(%rip),%rax        # 200fd0 <_ZSt4endlIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_@GLIBCXX_3.4>\r\n",
      " 8cd:\t48 89 c6             \tmov    %rax,%rsi\r\n",
      " 8d0:\t48 89 d7             \tmov    %rdx,%rdi\r\n",
      " 8d3:\te8 68 fe ff ff       \tcallq  740 <_ZNSolsEPFRSoS_E@plt>\r\n",
      " 8d8:\tb8 00 00 00 00       \tmov    $0x0,%eax\r\n",
      " 8dd:\tc9                   \tleaveq \r\n",
      " 8de:\tc3                   \tretq   \r\n"
     ]
    }
   ],
   "source": [
    "!objdump -d example0 | sed -n '/<main>:/,/retq/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to recognise what we wrote in C++ after a while... for example, lines 908, 90f and 916 define the three variables, lines 91d and 920 push the contents of the variables to the CPU registers, and finally line 923 adds the value of the two registers! For more complicated programmes, this becomes tricky to follow, to say the least... but this is also the point: this code is only supposed to be read by a machine. However, we do see that the critical part of our programme was converted to a handful of CPU instructions, which is great for performance. If you'd like to get more hands-on with assembly code, here are two useful resources:\n",
    "* https://www.agner.org/optimize: contains, amongst other things, a guide on optimization for x-86 processors (AMD, VIA and Intel). If you ever wondered what `mov`, `1ea` or `callq` do, what their throughput and latencies are on a given CPU, this is the place to go.\n",
    "* Intel publishes \"intrinsics\" i.e. C functions that map directly to individual CPU instructions. More here: https://software.intel.com/sites/landingpage/IntrinsicsGuide/#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreted languages, function differently from compiled ones. Instead of being converted to CPU instructions directly, the source code is parsed then intermediate, machine-independent *opcode* (or *bytecode*) is generated. This intermediate code is then *interpreted* by an interpreter (for the Python language, the reference interpreter is CPython - so called because it is written in C!). In a way, the interpreter is a \"machine within the machine\": it takes the place of the CPU (though of course it itself is being run by the CPU!). This explains some of the performance loss incurred when running Python scripts, since the presence of an interpreter induces overhead. We've written a python program equivalent to the C++ one above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./example1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can explicitly ask the Python executable to generate the intermediate opcode. Ever wondered what the `.pyc` files present in `__pycache__` were?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m compileall example1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python view_bytecode.py __pycache__/example1.cpython-36.pyc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group of opcodes directly corresponds to each line in `example1.py` (and this is much more readable than assembly code...). To execute each opcode, many CPU instructions are required - but how many? To get a feel for this, let's inspect the source code of the CPython interpreter! In fact, let's just consider the crucial opcode, which is `BINARY_ADD` (adds two numbers together). We'll have to clone the cpython repository... exciting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/python/cpython.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to have a closer look at ceval.c. We'll only show the relevant parts, since it is a lot of C code to take in in one go. Effectively, you'll find a big `switch` statement, followed by a lot of `case` statements, each corresponding to one opcode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat cpython/Python/ceval.c | grep 'case TARGET' | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is C code, it's not hard to see what this is doing: the programme is looping over each opcode, and based on the value of the opcode does someting different. So what happens when `BINARY_ADD` is encountered?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat cpython/Python/ceval.c | sed -n '/BINARY_ADD/,/DISPATCH()/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fairly complicated for a simple sum... but really all this does is call PyNumber_Add provided the arguments are not two Unicode strings (in which case they will be concatenated). So let's have a look at `PyNumber_Add`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat cpython/Objects/abstract.c | sed -n '/PyNumber_Add/,/return result/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we call `binary_op1`. If the result is `Py_NotImplemented` we do... something, else we return the result. Fine, let's have a look at `binary_op1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat cpython/Objects/abstract.c | sed -n '/binary_op1/,/Py_RETURN_NOTIMPLEMENTED/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be doing some type checking. If the object in question a \"number type\" (if so define `slotv` function...) and more checking, and finally if possible run slotv. And do make sure to check that the returned type is not `PyNotImplemented`, of course..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here is that Python is (very) dynamically typed: types are associated to *values* rather than *variables*. So whenever an opration is executed on an object, the interpreter has to check that the relevant object supports it. An take care of potential failures. Everything is an object in Python! What was one CPU instruction in our C++ example is quickly turning into many more when the Python interpreter is running the show. Because of the flexibility that Python offers (dynamic typing, etc), a lot of optimisations that are available to other interpreted languages (e.g. Java) are simply not available to it. That is the main reason why Python is so slow! It's common for a C++ version of a Python function to be 100x faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways in which you can improve the performance of your Python code. Normally, your programmes will spend most of their time in the same portion of the code - we call these *hotspots*. It is these hotspots that really need to be sped up, and not the rest of your code. For example, if your programme consists in reading a file from disk, parsing it into a custom data structure, and then running some bespoke ML algorithm on that structure, then probably only the latter step needs optimisation. We will discuss three different ways of speeding up your code: Cython, Numba and writing custom extensions; we won't discuss some others like using a different Python interpreter such as PyPy or Jython."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cython is an *optimising static compiler* for the Python language and the *extended Cython language, which is a superset of the Python programming language*. What does that mean? Cython compiles Python code, instead of interpreting it. Second, it defines a set of annotations that you may use on your Python code. These annotations are not part of the Python language. Rather, they help Cython understand the types of your variables and the signatures of your functions. Really, they let you write C code with Python syntax. The corollary here is that to use Cython well, you need to understand C. In particular, Cython is most succesful at optimising your code when you do not use Python's convenient features like dynamically typed variables, introspection, and so on. Let's give Cython a go. To compare its performance with pure Python, we'll test a function adding the first $10^8$ integers and returns the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat add_integers_python.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cython code should be written in files ending in '.pyx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat add_integers_cython.pyx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fairly similar to the pure Python function. The main difference is that we annoted the type of the variables using the `cdef` keyword, and also annotated the function arguments. Note that `uint64_t` stands for unsigned 64 bit integer. It can hold any non-negative integer between 0 and $2^{64} - 1$, inclusive. To compile `add_integers_cython.pyx` into a Python extension, we have written a custom `setup.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat setup.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the extension as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the pure python implementation to the Cython one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timefunc(func, *args, **kwargs):\n",
    "    t0 = time.time()\n",
    "    func(*args, **kwargs)\n",
    "    t1 = time.time()\n",
    "    return t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_integers_python import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(f, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_integers_cython import f  as f_cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(f_cython, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a roughly 200x improvement! You may want to check out `add_integers_cython.c`. The `add_integers_cython.pyx` file was tranlated into this C source file, and the latter was then compiled using a C compiler. For more information on Cython, see https://cython.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba is easier to use than Cython. Numba's premise is that rather then having you specify types manually, it will try to infer them at runtime for you. As a consequence, most of the time all you have to do is to decorate your functions to tell numba to try to attempt and optimise them. No need for a different language here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_numba = jit(nopython=True)(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_numba(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(f_numba, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That was fast. How is this possible? You may want to use the `inspect_asm()` or `inspect_llvm()` methods of `f_numba` to check your theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible, and sometimes necessary, to specify the signatures of your functions directly, rather than to let `numba` find out what they are on its own. That's how you do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import uint64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_numba_with_signature = jit(\n",
    "    uint64(uint64),\n",
    "    nopython=True\n",
    ")(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_numba_with_signature(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba has (very basic) support for classes, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jitclass\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jitclass(\n",
    "    OrderedDict([(\"x\", uint64), (\"y\", uint64)]),\n",
    ")\n",
    "class Point2D:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some trickery is required if you want to specify the signature of functions using your jitclassed classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumbaPoint2DType = Point2D.class_type.instance_type\n",
    "\n",
    "@jit(\n",
    "    NumbaPoint2DType(NumbaPoint2DType, NumbaPoint2DType),\n",
    "    nopython=True\n",
    ")\n",
    "def add_points(a, b):\n",
    "    return Point2D(a.x + b.x, a.y + b.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = add_points(Point2D(1, 2), Point2D(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(result.x, result.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class support is limited, however. For example, it is not possible for a class to reference itself - something you would do using pointers in C++. So defining recursive structures is very difficult (I have seen one example online, and could not adapt it to my needs). Again, knowing some C is useful here to understand what can and cannot be done with `numba`. For more information, follow the documentation here: http://numba.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using C/C++ directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If C knowledge is effectively required to properly use Cython or Numba, might we not want to code our functions directly in C? Indeed, this is possible, and is probably preferable. One of the risks in using Cython or Numba is that they may not always suit your needs (see limitations above). You would not want to realise this in the middle of a project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The C Foreign Function Interface (`cffi`) package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cffi` package allows you to use compiled C libraries directly. Here is our favourite function coded up in C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat add_integers_c.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat add_integers_c.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compile it into a shared library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<<<<< local\n",
    "!gcc -c -fpic add_integers_c.c\n",
    "=======\n",
    "!gcc -c -Wall -Werror -fPIC add_integers_c.c\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc -shared -Wl,-soname,libadd_integers_c.so -o libadd_integers_c.so add_integers_c.o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `cffi` to make functions from this shared library available from a Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {
     "scrolled": false
    },
    "remote_metadata": {
     "scrolled": true
    }
   },
   "outputs": [],
   "source": [
    "from cffi import FFI\n",
    "ffibuilder = FFI()\n",
    "\n",
    "# cdef() expects a single string declaring the C types, functions and\n",
    "# globals needed to use the shared object. It must be in valid C syntax.\n",
    "ffibuilder.cdef(\"\"\"\n",
    "    uint64_t f(uint64_t);\n",
    "\"\"\")\n",
    "\n",
    "# set_source() gives the name of the python extension module to\n",
    "# produce, and some C source code as a string.  This C code needs\n",
    "# to make the declarated functions, types and globals available,\n",
    "# so it is often just the \"#include\".\n",
    "<<<<<<< local\n",
    "ffibuilder.set_source(\n",
    "    \"_add_integers_cffi\",\n",
    "\"\"\"\n",
    "     #include \"add_integers_c.h\"   // the C header of the library\n",
    "\"\"\",\n",
    "     libraries=['add_integers_c'],\n",
    "    extra_link_args=['-L/project/performance/examples'] # Must add this to tell the linker where to find our shared library\n",
    ")   # library name, for the linker\n",
    "=======\n",
    "ffibuilder.set_source(\"_add_integers_cffi\",\n",
    "\"\"\"\n",
    "     #include \"add_integers_c.h\"   // the C header of the library\n",
    "\"\"\",\n",
    "     libraries=['add_integers_c'])   # library name, for the linker\n",
    ">>>>>>> remote\n",
    "\n",
    "ffibuilder.compile(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _add_integers_cffi import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.f(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a C++ extension with Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boost (https://www.boost.org) has a library which wraps all the boiler plate code required to write Python extensions manually (it will take care of reference counts for you, for example...). The result is that it is quite simple to expose your C++ classes as Python classes. Installation is a bit tricky, though. For reference, a list of instructions follows. However, we have included a script to automate this process. To run this script, close this notebook, open a new terminal and run\n",
    "```\n",
    "cd install-boost && ./install_boost.sh\n",
    "```\n",
    "This will restart Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual installation instructions:\n",
    "\n",
    "1. make sure that `numpy` is installed.  For example using `pip`: \n",
    "\n",
    "   ```\n",
    "   pip install numpy\n",
    "   ```\n",
    "\n",
    "2. Download the Boost C++ libraries version 1.69 available here:\n",
    "   https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.gz for\n",
    "   example by running\n",
    "    \n",
    "   ```\n",
    "   cd /tmp && wget https://dl.bintray.com/boostorg/release/1.69.0/source/boost_1_69_0.tar.gz\n",
    "   ```\n",
    "\n",
    "3. Extract the Boost C++ libraries:\n",
    "\n",
    "   ```\n",
    "   cd /tmp && tar -xvf boost_1_69_0.tar.gz\n",
    "   ``` \n",
    "   \n",
    "   This creates a directory `boost_1_69_0`.\n",
    "   Define an environment variable named `BOOST_ROOT`\n",
    "   pointing to the root of your newly installed Boost distribution:\n",
    "   \n",
    "   ```\n",
    "   export BOOST_ROOT=/tmp/boost_1_69_0\n",
    "   ```\n",
    "\n",
    "4. Compile the Boost Python and Numpy shared libraries. Run \n",
    "\n",
    "\t```\n",
    "\tcd $BOOST_ROOT && ./bootstrap.sh\n",
    "\t```\n",
    "\t\n",
    "   This will create a `bjam` configuration file named `project-config.jam`.\n",
    "   Bjam (the Boost build tool) is not always capable of detecting the correct\n",
    "   Python include paths, so we'll need to fix `project-config.jam` manually.\n",
    "   Look for a line resembling \n",
    "   ```\n",
    "   using python : 3.6 : /opt/anaconda/envs/Python3 ;\n",
    "   ```\n",
    "   in this file, and replace it with\n",
    "    \n",
    "   ```\n",
    "   using python : 3.6 : /opt/anaconda/envs/Python3 : /opt/anaconda/envs/Python3/include/python3.6m : /opt/anaconda/envs/Python3/lib ;\n",
    "   ``` \n",
    "   \n",
    "   If you are not using Faculty, if you are\n",
    "   targetting another version of Python, or if your Python installation is\n",
    "   located elsewhere, you will need to modify this step accordingly.  The\n",
    "   essential point is to make sure that the fourth field points to the directory\n",
    "   containing the Python C header files (in particular, it should contain the\n",
    "   file `Python.h`).  Now compile the Boost Python library with\n",
    "\n",
    "   ```\n",
    "   ./b2 install --with-python stage\n",
    "   ```\n",
    "   \n",
    "   The Boost Python and Numpy shared libraries will\n",
    "   be installed in `$BOOST_ROOT/stage/lib`.\n",
    "\n",
    "Let's also set a number of environment variables to make things tidier.\n",
    "\n",
    "* `BOOST_ROOT`: we have already defined this variable.\n",
    "  It should point to the directory containing the Boost C++ headers.\n",
    "\n",
    "* `BOOST_LIB`: this variable should point to the directory containing the shared\n",
    "  Boost Python and Boost Numpy libraries. To define it, run \n",
    "  \n",
    "  ```\n",
    "  export BOOST_LIB=$BOOST_ROOT/stage/lib\n",
    "  ```\n",
    "\n",
    "* `PYTHON_INCLUDE`: this variable should point to the directory containing the\n",
    "  Python header files. On Faculty, this would currently be set as follows:\n",
    "  \n",
    "  ```\n",
    "  export PYTHON_INCLUDE=/opt/anaconda/envs/Python3/include/python3.6m\n",
    "  ```\n",
    "\n",
    "* `NUMPY_INCLUDE`: this variable should point to the directory containing the\n",
    "  numpy header files.  On Faculty, this would currently be set as follows:\n",
    "\n",
    "  ```\n",
    "  export NUMPY_INCLUDE=/opt/anaconda/envs/Python3/lib/python3.6/site-packages/numpy/core/include\n",
    "  ```\n",
    "\n",
    "Additionally, `$BOOST_LIB` should be part of your `$LD_LIBRARY_PATH` environment\n",
    "variable. This is to make sure that the Python interpreter is able to find the\n",
    "shared Boost Python library we just compiled. Run the following\n",
    "command to make sure that `LD_LIBRARY_PATH` is correctly set:\n",
    "\n",
    "``` \n",
    "export LD_LIBRARY_PATH=$BOOST_LIB:$LD_LIBRARY_PATH\n",
    "```\n",
    "\n",
    "Note that this is the only environment variable that is required to be set\n",
    "after installation. On Faculty, you may want to create a file `envs.sh` in \n",
    "`/etc/sherlockml_environment.d` defining the variables above. Then restart Jupyter with \n",
    "```\n",
    "sudo sv stop jupyter && sudo sv start jupyter\n",
    "```\n",
    "The IPython kernels run by your Jupyter server will now have the variables correctly set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give Boost Python a go! Here is a simple example defining our function `f` as part of a module `add_integers_boost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat add_integers_boost.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ add_integers_boost.cpp \\\n",
    "     -fPIC \\\n",
    "     -shared \\\n",
    "     -I $BOOST_ROOT \\\n",
    "     -I $PYTHON_INCLUDE \\\n",
    "     -L $BOOST_LIB \\\n",
    "     -lboost_python36 \\\n",
    "     -Wno-deprecated-declarations \\\n",
    "     -o add_integers_boost.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word on the options:\n",
    "* `-fPIC` tells the compiler to generate position independent code, meaning that the code does not rely on where it is located in memory to be run. For example, jumps will be relative rather than absolute. This is required for shared libraries.\n",
    "* `-shared` tells the compiler to create a shared library.\n",
    "* `-I $BOOST_ROOT`, `-I $PYTHON_INCLUDE` and `-I $BOOST_LIB` give the compiler additional directories to look for header files\n",
    "* `-lboost_python36` tells the compiler to dynamically link the compiled library against the Boost Python library (for version 3.6)\n",
    "* `-Wno-deprecated-declarations` removes some warnings coming from the Boost headers (this can be omitted)\n",
    "* `-o add_integers_boost.so` specifies the output file name of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_integers_boost import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a C++ class exposed as a Python class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accumulator_boost.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ accumulator_boost.cpp \\\n",
    "     -fPIC \\\n",
    "     -shared \\\n",
    "     -I $BOOST_ROOT \\\n",
    "     -I $PYTHON_INCLUDE \\\n",
    "     -L $BOOST_LIB \\\n",
    "     -lboost_python36 \\\n",
    "     -Wno-deprecated-declarations \\\n",
    "     -o accumulator_boost.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accumulator_boost import Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accumulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.add(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You typically will only want to implement the critical code paths in C++ - and hence only part of your class in C++. Therefore you might want to wrap the Boost-generated class in another class. For examle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accumulator_boost import Accumulator as CoreAccumulator\n",
    "\n",
    "class Accumulator(CoreAccumulator):\n",
    "    def __init__(self):\n",
    "        self._core = CoreAccumulator()\n",
    "        \n",
    "    def add(self, increment):\n",
    "        self._core.add(increment)\n",
    "\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self._core.total\n",
    "\n",
    "    def help(self):\n",
    "        print(\"Use this class to accumulate numbers.\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Accumulator: total = {}\".format(self.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accumulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.add(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although quite bare, the Boost Python documentation does have a helpful tutorial which you might want to check out - see the Boost documentation. In particular, it will show you how to interact with Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can I code a C extension from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. Good luck :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about parallelisation... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System processes and threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing a *process* is an instance of program being executed. At any one time, your computer is running tens of processes, if not more. Here's how to get the number of running user processes on Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ps -A --no-headers | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you are very likely to be running (many) more processes than you have CPU cores! How is this possible? In effect, only one process is ever being run at a time on a machine with a single core. Processes *share CPU time*: the CPU constantly switches between them, and this gives the illusion that processes are being run concurrently. The part of your operating system responsible for orchestrating this is called the *scheduler*. It divides time into slices, and decides which process should be run during each slice. Of course, when you have more than one core, the scheduler will use them all: processes will be scheduled to run on different cores.\n",
    "\n",
    " A process can run one or more *threads*: these are sequences of execution sharing the same address space (so that communication between threads is much faster than communication between processes). So your system runs many processes, and each process can run many threads. Again, there is no relationship between number of cores and number of threads, but having more cores means that threads can run concurrently. To speed up the performance of you programmes, you might therefore want to make them *multithreaded*. This typically will make your programmes run faster on multi-core machines, but not always! It all depends on whether the bottleneck in the execution of your programme is the CPU, or something else. To give an example, if your programme is transcoding a video, then the bottleneck is the CPU - we say that it is *CPU-bound*. Video transcoder are typically multithreaded, and this significantly improved performance on multi-core machines. If your programme is downloading data from the web, then its performance is probably limited by the network bandwidth available to you - it is *I/O-bound*. In this context, the CPU is probably spending most of its time idling and waiting for packets to come from the source. Dividing the downloading task into two threads would probably not speed things up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `threading` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create threads in Python with the `threading` package. Run the following cells on a machine with at least two cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_integers_python import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_threaded():\n",
    "    return f(100000000) + f(100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(single_threaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_adapted(index, results, *args, **kwargs):\n",
    "    results[index] = f(*args, **kwargs)\n",
    "\n",
    "def multi_threaded():\n",
    "    results = {}\n",
    "    t0 = threading.Thread(target=f_adapted, args=(0, results, 100000000,))\n",
    "    t1 = threading.Thread(target=f_adapted, args=(1, results, 100000000,))\n",
    "    t0.start()\n",
    "    t1.start()\n",
    "    t0.join()\n",
    "    t1.join()\n",
    "    return results[0] + results[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(multi_threaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was not much faster than the single-threaded version... in fact, it may even be a bit slower! Here is the problem:\n",
    "Python threads *cannot be run concurrently*. This is a feature of the Python interpreter - the same programme written in C\n",
    "would indeed be twice faster than the single-threaded version. The Python interpreter has a so-called *Global Intepreter Lock (GIL)* - when a thread is being run by the interpreter, the GIL is set, which preventing other threads from running. In effect, the Python interpreter works like a single-core virtual machine. More on this here: https://www.dabeaz.com/python/UnderstandingGIL.pdf\n",
    "\n",
    "This does not mean that using threads in Python is not a good idea: they can be very useful if your programme has to run independent streams of code concurrently: for example to handle multiple connections to a server at the same time. But you shouldn't expect performance improvements on CPU-bound tasks from using Python threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `multiprocessing` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python does have a way of making use of multicore machines: the `multiprocessing` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_processed():\n",
    "    manager = multiprocessing.Manager()\n",
    "    results = manager.dict()\n",
    "    t0 = multiprocessing.Process(target=f_adapted, args=(0, results, 100000000,))\n",
    "    t1 = multiprocessing.Process(target=f_adapted, args=(1, results, 100000000,))\n",
    "    t0.start()\n",
    "    t1.start()\n",
    "    t0.join()\n",
    "    t1.join()\n",
    "    return results[0] + results[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefunc(multi_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *does* improve performance on multicore machines. As the name indicates, the way that this module works is by creating indepedent *processes* rather than *threads*: a distinct Python intepreter (and hence a distinct system process) will be run for each \"process\" that you create in this way. This means, in particular,  that these processes do not share the same address space. This works well only when a small amount of inter-process communication is required. Besides, running a new Python interpreter for each process has its own inconvenients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, if you choose to write a C/C++ extension (see above) then you can easily bypass the GIL mechanism, and you won't be limited by the `threading` or `multiprocessing` modules. And how does the (in?)famous `joblib` module come into the picture? Joblib will either use `threading` or `multiprocessing` depending on which backend you specify - and hence suffers from the same limitations as these modules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "7"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "4"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
